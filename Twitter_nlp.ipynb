{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0         4342\n",
       "1         3271\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[['target']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=train_data['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text']= train_data['text'].apply(lambda x : re.sub(r'http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text']= train_data[\"text\"].apply(lambda x : re.sub(r'[^\\w\\s]', '',x))\n",
    "train_data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Honda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text']=train_data['text'].apply(lambda x: [word for word in x.split() if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deeds', 'Reason', 'earthquake', 'May', 'ALLAH', 'Forgive', 'us']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stremmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: [stemmer.stem(word) for word in x if len(word) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [deed, reason, earthquak, may, allah, forgiv, us]\n",
       "1            [forest, fire, near, La, rong, sask, canada]\n",
       "2       [resid, ask, shelter, place, notifi, offic, ev...\n",
       "3       [13000, peopl, receiv, wildfir, evacu, order, ...\n",
       "4       [got, sent, photo, rubi, alaska, smoke, wildfi...\n",
       "                              ...                        \n",
       "7608    [two, giant, crane, hold, bridg, collaps, near...\n",
       "7609    [aria_ahrari, thetawniest, control, wild, fire...\n",
       "7610                [m194, 0104, utc5km, volcano, hawaii]\n",
       "7611    [polic, investig, ebik, collid, car, littl, po...\n",
       "7612    [latest, home, raze, northern, california, wil...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokneizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target=train_data[\"target\"]\n",
    "\n",
    "x_train, x_test ,y_train ,y_test = train_test_split(train_data['text'],target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"oov\")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "tokenized_train = tokenizer.texts_to_matrix(x_train)\n",
    "tokenized_test = tokenizer.texts_to_matrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 12977)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenized_train.shape[1]\n",
    "tokenized_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1661184   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,671,553\n",
      "Trainable params: 1,671,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(vocab_size,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer= RMSprop(lr=0.0001),\n",
    "              loss= keras.losses.binary_crossentropy,\n",
    "              metrics= [keras.metrics.binary_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "191/191 [==============================] - 5s 21ms/step - loss: 0.6880 - binary_accuracy: 0.6286 - val_loss: 0.6765 - val_binary_accuracy: 0.7242\n",
      "Epoch 2/6\n",
      "191/191 [==============================] - 4s 21ms/step - loss: 0.6506 - binary_accuracy: 0.7969 - val_loss: 0.6181 - val_binary_accuracy: 0.8037\n",
      "Epoch 3/6\n",
      "191/191 [==============================] - 4s 21ms/step - loss: 0.5562 - binary_accuracy: 0.8365 - val_loss: 0.5143 - val_binary_accuracy: 0.8168\n",
      "Epoch 4/6\n",
      "191/191 [==============================] - 4s 21ms/step - loss: 0.4411 - binary_accuracy: 0.8504 - val_loss: 0.4396 - val_binary_accuracy: 0.8188\n",
      "Epoch 5/6\n",
      "191/191 [==============================] - 4s 22ms/step - loss: 0.3614 - binary_accuracy: 0.8647 - val_loss: 0.4150 - val_binary_accuracy: 0.8181\n",
      "Epoch 6/6\n",
      "191/191 [==============================] - 4s 20ms/step - loss: 0.3151 - binary_accuracy: 0.8806 - val_loss: 0.4144 - val_binary_accuracy: 0.8207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2434dd1d820>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tokenized_train,y_train,epochs=6,validation_data=(tokenized_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_seq = tokenizer.texts_to_sequences(x_train)\n",
    "tokenized_seq_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_seq = pad_sequences(tokenizer_seq, maxlen=25, padding='post',truncating='post')\n",
    "tokenized_seq_test = pad_sequences(tokenized_seq_test, maxlen=25, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12976"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = models.Sequential()\n",
    "model_lstm.add(layers.Embedding(vocab_size,64))\n",
    "model_lstm.add(layers.Dropout(0.4))\n",
    "model_lstm.add(layers.LSTM(64))\n",
    "model_lstm.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, None, 64)          830528    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 863,617\n",
      "Trainable params: 863,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.compile(optimizer=RMSprop(lr=0.0001), loss='binary_crossentropy', metrics=['acc'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 7s 24ms/step - loss: 0.6851 - acc: 0.5663 - val_loss: 0.6806 - val_acc: 0.5732\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.6787 - acc: 0.5696 - val_loss: 0.6722 - val_acc: 0.5732\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 4s 20ms/step - loss: 0.6135 - acc: 0.6501 - val_loss: 0.5305 - val_acc: 0.7676\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.5010 - acc: 0.7803 - val_loss: 0.4781 - val_acc: 0.7754\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 4s 20ms/step - loss: 0.4563 - acc: 0.7908 - val_loss: 0.4558 - val_acc: 0.7787\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 4s 20ms/step - loss: 0.4297 - acc: 0.8076 - val_loss: 0.4373 - val_acc: 0.7945\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.4047 - acc: 0.8235 - val_loss: 0.4395 - val_acc: 0.7827\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 4s 21ms/step - loss: 0.3905 - acc: 0.8322 - val_loss: 0.4359 - val_acc: 0.8083\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 4s 20ms/step - loss: 0.3720 - acc: 0.8424 - val_loss: 0.4233 - val_acc: 0.8070\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.3636 - acc: 0.8435 - val_loss: 0.4205 - val_acc: 0.8148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24359798640>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(tokenizer_seq,y_train,epochs=10,validation_data=(tokenized_seq_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = models.Sequential()\n",
    "model_gru.add(layers.Embedding(vocab_size,128))\n",
    "model_gru.add(layers.Dropout(0.2))\n",
    "model_gru.add(layers.Bidirectional(layers.GRU(128,return_sequences=True)))\n",
    "model_gru.add(layers.Dropout(0.2))\n",
    "model_gru.add(layers.Bidirectional(layers.GRU(64)))\n",
    "model_gru.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 63s 297ms/step - loss: 0.5180 - acc: 0.7440 - val_loss: 0.4178 - val_acc: 0.8148\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 54s 285ms/step - loss: 0.2791 - acc: 0.8913 - val_loss: 0.4658 - val_acc: 0.7965\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 55s 287ms/step - loss: 0.1481 - acc: 0.9442 - val_loss: 0.5534 - val_acc: 0.8017\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 57s 299ms/step - loss: 0.0963 - acc: 0.9716 - val_loss: 0.5658 - val_acc: 0.7814\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 68s 354ms/step - loss: 0.0768 - acc: 0.9742 - val_loss: 0.6051 - val_acc: 0.7925\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 68s 357ms/step - loss: 0.0680 - acc: 0.9773 - val_loss: 0.6086 - val_acc: 0.7866\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 65s 341ms/step - loss: 0.0553 - acc: 0.9821 - val_loss: 0.6032 - val_acc: 0.7741\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 65s 343ms/step - loss: 0.0517 - acc: 0.9803 - val_loss: 0.6530 - val_acc: 0.7833\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 66s 347ms/step - loss: 0.0485 - acc: 0.9819 - val_loss: 0.6631 - val_acc: 0.7787\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 68s 356ms/step - loss: 0.0435 - acc: 0.9821 - val_loss: 0.7145 - val_acc: 0.7814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243538ffa60>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_gru.fit(tokenizer_seq,y_train,epochs=10,validation_data=(tokenized_seq_test,y_test),verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = models.Sequential()\n",
    "model_lstm.add(layers.Embedding(vocab_size,128))\n",
    "model_lstm.add(layers.Dropout(0.4))\n",
    "model_lstm.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "model_lstm.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 13s 41ms/step - loss: 0.6806 - acc: 0.5695 - val_loss: 0.6715 - val_acc: 0.5732\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 7s 36ms/step - loss: 0.6638 - acc: 0.5736 - val_loss: 0.6505 - val_acc: 0.6093\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 7s 37ms/step - loss: 0.6240 - acc: 0.6777 - val_loss: 0.5632 - val_acc: 0.7603\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 7s 37ms/step - loss: 0.4711 - acc: 0.7900 - val_loss: 0.4540 - val_acc: 0.7978\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 7s 38ms/step - loss: 0.4219 - acc: 0.8159 - val_loss: 0.4409 - val_acc: 0.7997\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 7s 37ms/step - loss: 0.3956 - acc: 0.8291 - val_loss: 0.4326 - val_acc: 0.8096\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 7s 38ms/step - loss: 0.3813 - acc: 0.8379 - val_loss: 0.4226 - val_acc: 0.8155\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 8s 40ms/step - loss: 0.3662 - acc: 0.8470 - val_loss: 0.4293 - val_acc: 0.8096\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 7s 37ms/step - loss: 0.3535 - acc: 0.8525 - val_loss: 0.4207 - val_acc: 0.8168\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 7s 37ms/step - loss: 0.3413 - acc: 0.8604 - val_loss: 0.4237 - val_acc: 0.8135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2435591fbb0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.compile(optimizer=RMSprop(lr=0.0001), loss='binary_crossentropy', metrics=['acc'])\n",
    "model_lstm.fit(tokenizer_seq,y_train,epochs=10,validation_data=(tokenized_seq_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
