{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd7ff44",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "235cc811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Honda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow \n",
    "import keras \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer \n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b469c3",
   "metadata": {},
   "source": [
    "# Read_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "298a96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('dataset.csv')\n",
    "Data_prep_2=Data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d380b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.drop(columns='id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a5b9013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0   @user when a father is dysfunctional and is s...\n",
       "1      0  @user @user thanks for #lyft credit i can't us...\n",
       "2      0                                bihday your majesty\n",
       "3      0  #model   i love u take with u all the time in ...\n",
       "4      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098128a",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba918ea",
   "metadata": {},
   "source": [
    "Check_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "931b8243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c8dcb",
   "metadata": {},
   "source": [
    "Check_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4ad7734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    False\n",
       "tweet    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e2a55",
   "metadata": {},
   "source": [
    "Check_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37c41cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "311edd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64ccf4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e86e1",
   "metadata": {},
   "source": [
    "As baseline processing we will do so :\n",
    "\n",
    "1-check for mails to remove\n",
    "\n",
    "2-check for websites to remove\n",
    " \n",
    "3-remove non chars\n",
    "\n",
    "4-Normalization\n",
    "\n",
    "5-remove stop words\n",
    "\n",
    "6-Lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ee0bfda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet'].apply(lambda x: re.findall('\\S+@\\S+',x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e546891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         @user when a father is dysfunctional and is s...\n",
       "1        @user @user thanks for #lyft credit i can't us...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow @user carnt wait first ti...\n",
       "31957    ate @user isz that youuu?ðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you @user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: re.sub('\\S+@\\S+',' ',x))\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "042ff80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet'].apply(lambda x: re.findall('http\\S+',x)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47bef173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          user when a father is dysfunctional and is s...\n",
       "1         user  user thanks for  lyft credit i can t us...\n",
       "2                                      bihday your majesty\n",
       "3         model   i love u take with u all the time in ...\n",
       "4                   factsguide  society now     motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow  user carnt wait first ti...\n",
       "31957    ate  user isz that youuu                      ...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you  user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x:re.sub('[^A-Za-z0-9]',' ',x))\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e4a90d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7adc354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          user when a father is dysfunctional and is s...\n",
       "1         user  user thanks for  lyft credit i can t us...\n",
       "2                                      bihday your majesty\n",
       "3         model   i love u take with u all the time in ...\n",
       "4                   factsguide  society now     motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow  user carnt wait first ti...\n",
       "31957    ate  user isz that youuu                      ...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you  user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4c55eb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          user when a father is dysfunctional and is s...\n",
       "1         user  user thanks for  lyft credit i can t us...\n",
       "2                                      bihday your majesty\n",
       "3         model   i love u take with u all the time in ...\n",
       "4                   factsguide  society now     motivation\n",
       "                               ...                        \n",
       "31956    off fishing tomorrow  user carnt wait first ti...\n",
       "31957    ate  user isz that youuu                      ...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31961                     thank you  user for you follow  \n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: re.sub('\\d+','',x))\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "33d53624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Honda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b4fb3cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [user, father, dysfunctional, selfish, drags, ...\n",
       "1        [user, user, thanks, lyft, credit, use, cause,...\n",
       "2                                        [bihday, majesty]\n",
       "3                      [model, love, u, take, u, time, ur]\n",
       "4                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "31956    [fishing, tomorrow, user, carnt, wait, first, ...\n",
       "31957                              [ate, user, isz, youuu]\n",
       "31958    [see, nina, turner, airwaves, trying, wrap, ma...\n",
       "31959    [listening, sad, songs, monday, morning, otw, ...\n",
       "31961                                [thank, user, follow]\n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x : [word for word in x.split()  if word not in stop_words])\n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0f1ed4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [user, father, dysfunctional, selfish, drag, k...\n",
       "1        [user, user, thank, lyft, credit, use, cause, ...\n",
       "2                                        [bihday, majesty]\n",
       "3                      [model, love, u, take, u, time, ur]\n",
       "4                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "31956    [fish, tomorrow, user, carnt, wait, first, tim...\n",
       "31957                              [eat, user, isz, youuu]\n",
       "31958    [see, nina, turner, airwave, try, wrap, mantle...\n",
       "31959    [listen, sad, song, monday, morning, otw, work...\n",
       "31961                                [thank, user, follow]\n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags=[wordnet.VERB,wordnet.ADJ,wordnet.ADV,wordnet.NOUN]\n",
    "lemmitaizer=WordNetLemmatizer()\n",
    "for pos in pos_tags:\n",
    "    Data['tweet']=Data['tweet'].apply(lambda x: [lemmitaizer.lemmatize(word,pos=pos) for word in x])\n",
    "    \n",
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2130b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['tweet']=Data['tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f85049e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        user father dysfunctional selfish drag kid dys...\n",
       "1        user user thank lyft credit use cause offer wh...\n",
       "2                                           bihday majesty\n",
       "3                              model love u take u time ur\n",
       "4                            factsguide society motivation\n",
       "                               ...                        \n",
       "31956        fish tomorrow user carnt wait first time year\n",
       "31957                                   eat user isz youuu\n",
       "31958    see nina turner airwave try wrap mantle genuin...\n",
       "31959          listen sad song monday morning otw work sad\n",
       "31961                                    thank user follow\n",
       "Name: tweet, Length: 29530, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "139dfed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.to_csv(\"tweets_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e530c",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3c4dc",
   "metadata": {},
   "source": [
    "1-count vectorizer\n",
    "\n",
    "2-tfidf\n",
    "\n",
    "3-tokenizer(binary,count,freq)\n",
    "\n",
    "4-pretrained(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6e398312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target=Data['label']\n",
    "x_train, x_test ,y_train ,y_test = train_test_split(Data['tweet'],target,test_size=0.1,stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3343bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8cd414a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(max_features=10000,ngram_range=(1,1))\n",
    "count_vect.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa1b1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9959"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_['york']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd503e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f0e533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count=count_vect.transform(x_train).todense()\n",
    "X_test_count=count_vect.transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a166fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abasel</th>\n",
       "      <th>abba</th>\n",
       "      <th>abc</th>\n",
       "      <th>abe</th>\n",
       "      <th>ability</th>\n",
       "      <th>abitur</th>\n",
       "      <th>...</th>\n",
       "      <th>zionazis</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zootopia</th>\n",
       "      <th>zoro</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>liv know weird yeahhh bread get hole stoner sistabanta</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one today believe bihday timeflies</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel sunday family travel couple family life chiangmai thailand</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure time day leisure leisuretime bangsean milksshop cocoa bos</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love findingdory wait movie even find dory</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thankful taxi thankful positive</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life measure number breath take moment take breath away maya angelou love woman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everyday caturday caturday cat saturday indraloka weekend</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reason millennials work keith breene user challenge employment</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yep definitely mean meanmuggin stare cat animallovers lol lmao bruh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26577 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    aap  aaron  ab  abandon  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...    0      0   0        0   \n",
       "one today believe bihday timeflies                    0      0   0        0   \n",
       "travel sunday family travel couple family life ...    0      0   0        0   \n",
       "leisure time day leisure leisuretime bangsean m...    0      0   0        0   \n",
       "love findingdory wait movie even find dory            0      0   0        0   \n",
       "...                                                 ...    ...  ..      ...   \n",
       "thankful taxi thankful positive                       0      0   0        0   \n",
       "life measure number breath take moment take bre...    0      0   0        0   \n",
       "everyday caturday caturday cat saturday indralo...    0      0   0        0   \n",
       "reason millennials work keith breene user chall...    0      0   0        0   \n",
       "yep definitely mean meanmuggin stare cat animal...    0      0   0        0   \n",
       "\n",
       "                                                    abasel  abba  abc  abe  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...       0     0    0    0   \n",
       "one today believe bihday timeflies                       0     0    0    0   \n",
       "travel sunday family travel couple family life ...       0     0    0    0   \n",
       "leisure time day leisure leisuretime bangsean m...       0     0    0    0   \n",
       "love findingdory wait movie even find dory               0     0    0    0   \n",
       "...                                                    ...   ...  ...  ...   \n",
       "thankful taxi thankful positive                          0     0    0    0   \n",
       "life measure number breath take moment take bre...       0     0    0    0   \n",
       "everyday caturday caturday cat saturday indralo...       0     0    0    0   \n",
       "reason millennials work keith breene user chall...       0     0    0    0   \n",
       "yep definitely mean meanmuggin stare cat animal...       0     0    0    0   \n",
       "\n",
       "                                                    ability  abitur  ...  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...        0       0  ...   \n",
       "one today believe bihday timeflies                        0       0  ...   \n",
       "travel sunday family travel couple family life ...        0       0  ...   \n",
       "leisure time day leisure leisuretime bangsean m...        0       0  ...   \n",
       "love findingdory wait movie even find dory                0       0  ...   \n",
       "...                                                     ...     ...  ...   \n",
       "thankful taxi thankful positive                           0       0  ...   \n",
       "life measure number breath take moment take bre...        0       0  ...   \n",
       "everyday caturday caturday cat saturday indralo...        0       0  ...   \n",
       "reason millennials work keith breene user chall...        0       0  ...   \n",
       "yep definitely mean meanmuggin stare cat animal...        0       0  ...   \n",
       "\n",
       "                                                    zionazis  zionism  zombie  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...         0        0       0   \n",
       "one today believe bihday timeflies                         0        0       0   \n",
       "travel sunday family travel couple family life ...         0        0       0   \n",
       "leisure time day leisure leisuretime bangsean m...         0        0       0   \n",
       "love findingdory wait movie even find dory                 0        0       0   \n",
       "...                                                      ...      ...     ...   \n",
       "thankful taxi thankful positive                            0        0       0   \n",
       "life measure number breath take moment take bre...         0        0       0   \n",
       "everyday caturday caturday cat saturday indralo...         0        0       0   \n",
       "reason millennials work keith breene user chall...         0        0       0   \n",
       "yep definitely mean meanmuggin stare cat animal...         0        0       0   \n",
       "\n",
       "                                                    zomg  zone  zoo  zootopia  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...     0     0    0         0   \n",
       "one today believe bihday timeflies                     0     0    0         0   \n",
       "travel sunday family travel couple family life ...     0     0    0         0   \n",
       "leisure time day leisure leisuretime bangsean m...     0     0    0         0   \n",
       "love findingdory wait movie even find dory             0     0    0         0   \n",
       "...                                                  ...   ...  ...       ...   \n",
       "thankful taxi thankful positive                        0     0    0         0   \n",
       "life measure number breath take moment take bre...     0     0    0         0   \n",
       "everyday caturday caturday cat saturday indralo...     0     0    0         0   \n",
       "reason millennials work keith breene user chall...     0     0    0         0   \n",
       "yep definitely mean meanmuggin stare cat animal...     0     0    0         0   \n",
       "\n",
       "                                                    zoro  zuma  zzzzzzzz  \n",
       "liv know weird yeahhh bread get hole stoner sis...     0     0         0  \n",
       "one today believe bihday timeflies                     0     0         0  \n",
       "travel sunday family travel couple family life ...     0     0         0  \n",
       "leisure time day leisure leisuretime bangsean m...     0     0         0  \n",
       "love findingdory wait movie even find dory             0     0         0  \n",
       "...                                                  ...   ...       ...  \n",
       "thankful taxi thankful positive                        0     0         0  \n",
       "life measure number breath take moment take bre...     0     0         0  \n",
       "everyday caturday caturday cat saturday indralo...     0     0         0  \n",
       "reason millennials work keith breene user chall...     0     0         0  \n",
       "yep definitely mean meanmuggin stare cat animal...     0     0         0  \n",
       "\n",
       "[26577 rows x 10000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_count,index=x_train,columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6a6f074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26577, 10000)\n",
      "(2953, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_count.shape)\n",
    "print(X_test_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fe07aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "model_1=models.Sequential()\n",
    "model_1.add(layers.Dense(128,activation='relu',input_shape=(10000,)))\n",
    "model_1.add(layers.Dense(64,activation='relu'))\n",
    "model_1.add(layers.Dense(32,activation='relu'))\n",
    "model_1.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df24ff34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model_1.compile(optimizer= RMSprop(lr=0.0001),\n",
    "              loss= keras.losses.binary_crossentropy,\n",
    "              metrics= [keras.metrics.binary_accuracy,keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adcc7819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "831/831 [==============================] - 14s 17ms/step - loss: 0.1083 - binary_accuracy: 0.9666 - auc: 0.9442 - val_loss: 0.1557 - val_binary_accuracy: 0.9553 - val_auc: 0.8791\n",
      "Epoch 2/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.1035 - binary_accuracy: 0.9689 - auc: 0.9477 - val_loss: 0.1524 - val_binary_accuracy: 0.9570 - val_auc: 0.8901\n",
      "Epoch 3/10\n",
      "831/831 [==============================] - 15s 18ms/step - loss: 0.0984 - binary_accuracy: 0.9708 - auc: 0.9517 - val_loss: 0.1522 - val_binary_accuracy: 0.9577 - val_auc: 0.8869\n",
      "Epoch 4/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.0935 - binary_accuracy: 0.9725 - auc: 0.9557 - val_loss: 0.1492 - val_binary_accuracy: 0.9594 - val_auc: 0.8946\n",
      "Epoch 5/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.0900 - binary_accuracy: 0.9742 - auc: 0.9577 - val_loss: 0.1509 - val_binary_accuracy: 0.9583 - val_auc: 0.8977\n",
      "Epoch 6/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.0859 - binary_accuracy: 0.9754 - auc: 0.9622 - val_loss: 0.1527 - val_binary_accuracy: 0.9594 - val_auc: 0.8917\n",
      "Epoch 7/10\n",
      "831/831 [==============================] - 14s 17ms/step - loss: 0.0815 - binary_accuracy: 0.9768 - auc: 0.9638 - val_loss: 0.1527 - val_binary_accuracy: 0.9583 - val_auc: 0.8988\n",
      "Epoch 8/10\n",
      "831/831 [==============================] - 15s 18ms/step - loss: 0.0784 - binary_accuracy: 0.9779 - auc: 0.9670 - val_loss: 0.1520 - val_binary_accuracy: 0.9597 - val_auc: 0.9031\n",
      "Epoch 9/10\n",
      "831/831 [==============================] - 15s 18ms/step - loss: 0.0746 - binary_accuracy: 0.9785 - auc: 0.9692 - val_loss: 0.1591 - val_binary_accuracy: 0.9587 - val_auc: 0.8994\n",
      "Epoch 10/10\n",
      "831/831 [==============================] - 15s 18ms/step - loss: 0.0716 - binary_accuracy: 0.9797 - auc: 0.9715 - val_loss: 0.1582 - val_binary_accuracy: 0.9594 - val_auc: 0.8996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b0e359b50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_train_count,y_train,epochs=10,validation_data=(X_test_count,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9864bd9",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c90d5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=10000,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea13117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'user': 2,\n",
       " 'day': 3,\n",
       " 'love': 4,\n",
       " 'get': 5,\n",
       " 'happy': 6,\n",
       " 'amp': 7,\n",
       " 'go': 8,\n",
       " 'make': 9,\n",
       " 'life': 10,\n",
       " 'today': 11,\n",
       " 'like': 12,\n",
       " 'u': 13,\n",
       " 'new': 14,\n",
       " 'good': 15,\n",
       " 'father': 16,\n",
       " 'see': 17,\n",
       " 'time': 18,\n",
       " 'smile': 19,\n",
       " 'people': 20,\n",
       " 'one': 21,\n",
       " 'bihday': 22,\n",
       " 'friend': 23,\n",
       " 'come': 24,\n",
       " 'feel': 25,\n",
       " 'look': 26,\n",
       " 'work': 27,\n",
       " 'want': 28,\n",
       " 'wait': 29,\n",
       " 'girl': 30,\n",
       " 'weekend': 31,\n",
       " 'thank': 32,\n",
       " 'week': 33,\n",
       " 'fun': 34,\n",
       " 'think': 35,\n",
       " 'family': 36,\n",
       " 'summer': 37,\n",
       " 'need': 38,\n",
       " 'know': 39,\n",
       " 'year': 40,\n",
       " 'great': 41,\n",
       " 'say': 42,\n",
       " 'thankful': 43,\n",
       " 'live': 44,\n",
       " 'friday': 45,\n",
       " 'positive': 46,\n",
       " 'first': 47,\n",
       " 'morning': 48,\n",
       " 'world': 49,\n",
       " 'beautiful': 50,\n",
       " 'back': 51,\n",
       " 'tomorrow': 52,\n",
       " 'best': 53,\n",
       " 'thing': 54,\n",
       " 'dad': 55,\n",
       " 'take': 56,\n",
       " 'watch': 57,\n",
       " 'really': 58,\n",
       " 'way': 59,\n",
       " 'even': 60,\n",
       " 'home': 61,\n",
       " 'sad': 62,\n",
       " 'orlando': 63,\n",
       " 'never': 64,\n",
       " 'fathersday': 65,\n",
       " 'blog': 66,\n",
       " 'night': 67,\n",
       " 'sunday': 68,\n",
       " 'music': 69,\n",
       " 'trump': 70,\n",
       " 'happiness': 71,\n",
       " 'right': 72,\n",
       " 'find': 73,\n",
       " 'cute': 74,\n",
       " 'much': 75,\n",
       " 'play': 76,\n",
       " 'leave': 77,\n",
       " 'bless': 78,\n",
       " 'pay': 79,\n",
       " 'always': 80,\n",
       " 'show': 81,\n",
       " 'last': 82,\n",
       " 'follow': 83,\n",
       " 'amaze': 84,\n",
       " 'next': 85,\n",
       " 'game': 86,\n",
       " 'let': 87,\n",
       " 'healthy': 88,\n",
       " 'still': 89,\n",
       " 'gold': 90,\n",
       " 'give': 91,\n",
       " 'tonight': 92,\n",
       " 'silver': 93,\n",
       " 'woman': 94,\n",
       " 'selfie': 95,\n",
       " 'ready': 96,\n",
       " 'guy': 97,\n",
       " 'enjoy': 98,\n",
       " 'many': 99,\n",
       " 'little': 100,\n",
       " 'quote': 101,\n",
       " 'man': 102,\n",
       " 'wish': 103,\n",
       " 'keep': 104,\n",
       " 'excite': 105,\n",
       " 'everyone': 106,\n",
       " 'miss': 107,\n",
       " 'baby': 108,\n",
       " 'forex': 109,\n",
       " 'would': 110,\n",
       " 'another': 111,\n",
       " 'lose': 112,\n",
       " 'kid': 113,\n",
       " 'try': 114,\n",
       " 'ever': 115,\n",
       " 'dog': 116,\n",
       " 'via': 117,\n",
       " 'book': 118,\n",
       " 'well': 119,\n",
       " 'finally': 120,\n",
       " 'god': 121,\n",
       " 'bull': 122,\n",
       " 'hope': 123,\n",
       " 'black': 124,\n",
       " 'happen': 125,\n",
       " 'bad': 126,\n",
       " 'stop': 127,\n",
       " 'big': 128,\n",
       " 'food': 129,\n",
       " 'white': 130,\n",
       " 'call': 131,\n",
       " 'long': 132,\n",
       " 'sta': 133,\n",
       " 'hate': 134,\n",
       " 'saturday': 135,\n",
       " 'news': 136,\n",
       " 'sun': 137,\n",
       " 'boy': 138,\n",
       " 'hour': 139,\n",
       " 'shoot': 140,\n",
       " 'someone': 141,\n",
       " 'end': 142,\n",
       " 'late': 143,\n",
       " 'video': 144,\n",
       " 'school': 145,\n",
       " 'every': 146,\n",
       " 'change': 147,\n",
       " 'meet': 148,\n",
       " 'funny': 149,\n",
       " 'th': 150,\n",
       " 'use': 151,\n",
       " 'place': 152,\n",
       " 'believe': 153,\n",
       " 'free': 154,\n",
       " 'gt': 155,\n",
       " 'travel': 156,\n",
       " 'old': 157,\n",
       " 'lol': 158,\n",
       " 'holiday': 159,\n",
       " 'break': 160,\n",
       " 'month': 161,\n",
       " 'run': 162,\n",
       " 'read': 163,\n",
       " 'n': 164,\n",
       " 'child': 165,\n",
       " 'help': 166,\n",
       " 'may': 167,\n",
       " 'face': 168,\n",
       " 'yes': 169,\n",
       " 'fuck': 170,\n",
       " 'nothing': 171,\n",
       " 'bear': 172,\n",
       " 'please': 173,\n",
       " 'euro': 174,\n",
       " 'nice': 175,\n",
       " 'awesome': 176,\n",
       " 'cool': 177,\n",
       " 'beach': 178,\n",
       " 'w': 179,\n",
       " 'team': 180,\n",
       " 'b': 181,\n",
       " 'do': 182,\n",
       " 'peace': 183,\n",
       " 'shop': 184,\n",
       " 'soon': 185,\n",
       " 'city': 186,\n",
       " 'word': 187,\n",
       " 'race': 188,\n",
       " 'hea': 189,\n",
       " 'america': 190,\n",
       " 'instagood': 191,\n",
       " 'tell': 192,\n",
       " 'lt': 193,\n",
       " 'proud': 194,\n",
       " 'monday': 195,\n",
       " 'hear': 196,\n",
       " 'r': 197,\n",
       " 'affirmation': 198,\n",
       " 'two': 199,\n",
       " 'kill': 200,\n",
       " 'win': 201,\n",
       " 'job': 202,\n",
       " 'lot': 203,\n",
       " 'true': 204,\n",
       " 'wed': 205,\n",
       " 'check': 206,\n",
       " 'talk': 207,\n",
       " 'dance': 208,\n",
       " 'tweet': 209,\n",
       " 'open': 210,\n",
       " 'june': 211,\n",
       " 'post': 212,\n",
       " 'moment': 213,\n",
       " 'forward': 214,\n",
       " 'dream': 215,\n",
       " 'real': 216,\n",
       " 'away': 217,\n",
       " 'stay': 218,\n",
       " 'something': 219,\n",
       " 'fan': 220,\n",
       " 'around': 221,\n",
       " 'person': 222,\n",
       " 'hard': 223,\n",
       " 'hair': 224,\n",
       " 'lovely': 225,\n",
       " 'oh': 226,\n",
       " 'e': 227,\n",
       " 'buy': 228,\n",
       " 'climb': 229,\n",
       " 'motivation': 230,\n",
       " 'relax': 231,\n",
       " 'season': 232,\n",
       " 'photo': 233,\n",
       " 'head': 234,\n",
       " 'without': 235,\n",
       " 'attack': 236,\n",
       " 'twitter': 237,\n",
       " 'fashion': 238,\n",
       " 'song': 239,\n",
       " 'strong': 240,\n",
       " 'mean': 241,\n",
       " 'hot': 242,\n",
       " 'sleep': 243,\n",
       " 'pa': 244,\n",
       " 'gun': 245,\n",
       " 'rip': 246,\n",
       " 'could': 247,\n",
       " 'cry': 248,\n",
       " 'flower': 249,\n",
       " 'angry': 250,\n",
       " 'listen': 251,\n",
       " 'share': 252,\n",
       " 'p': 253,\n",
       " 'drink': 254,\n",
       " 'everything': 255,\n",
       " 'gay': 256,\n",
       " 'cat': 257,\n",
       " 'house': 258,\n",
       " 'gonna': 259,\n",
       " 'mom': 260,\n",
       " 'die': 261,\n",
       " 'wow': 262,\n",
       " 'movie': 263,\n",
       " 'im': 264,\n",
       " 'put': 265,\n",
       " 'sex': 266,\n",
       " 'walk': 267,\n",
       " 'racist': 268,\n",
       " 'celebrate': 269,\n",
       " 'st': 270,\n",
       " 'young': 271,\n",
       " 'yet': 272,\n",
       " 'country': 273,\n",
       " 'move': 274,\n",
       " 'whatever': 275,\n",
       " 'london': 276,\n",
       " 'money': 277,\n",
       " 'tear': 278,\n",
       " 'story': 279,\n",
       " 'beauty': 280,\n",
       " 'ur': 281,\n",
       " 'pretty': 282,\n",
       " 'order': 283,\n",
       " 'full': 284,\n",
       " 'obama': 285,\n",
       " 'truth': 286,\n",
       " 'grateful': 287,\n",
       " 'men': 288,\n",
       " 'already': 289,\n",
       " 'mind': 290,\n",
       " 'v': 291,\n",
       " 'care': 292,\n",
       " 'thursday': 293,\n",
       " 'shit': 294,\n",
       " 'ticket': 295,\n",
       " 'polar': 296,\n",
       " 'sexy': 297,\n",
       " 'laugh': 298,\n",
       " 'omg': 299,\n",
       " 'eat': 300,\n",
       " 'couple': 301,\n",
       " 'learn': 302,\n",
       " 'turn': 303,\n",
       " 'mood': 304,\n",
       " 'joy': 305,\n",
       " 'photooftheday': 306,\n",
       " 'pic': 307,\n",
       " 'bring': 308,\n",
       " 'become': 309,\n",
       " 'american': 310,\n",
       " 'health': 311,\n",
       " 'super': 312,\n",
       " 'x': 313,\n",
       " 'followme': 314,\n",
       " 'close': 315,\n",
       " 'fitness': 316,\n",
       " 'perfect': 317,\n",
       " 'success': 318,\n",
       " 'since': 319,\n",
       " 'finish': 320,\n",
       " 'also': 321,\n",
       " 'inspiration': 322,\n",
       " 'medium': 323,\n",
       " 'business': 324,\n",
       " 'pm': 325,\n",
       " 'high': 326,\n",
       " 'wonderful': 327,\n",
       " 'son': 328,\n",
       " 'remember': 329,\n",
       " 'plan': 330,\n",
       " 'arrive': 331,\n",
       " 'wednesday': 332,\n",
       " 'sweet': 333,\n",
       " 'yay': 334,\n",
       " 'crazy': 335,\n",
       " 'event': 336,\n",
       " 'alone': 337,\n",
       " 'dead': 338,\n",
       " 'hey': 339,\n",
       " 'must': 340,\n",
       " 'picture': 341,\n",
       " 'rest': 342,\n",
       " 'car': 343,\n",
       " 'top': 344,\n",
       " 'set': 345,\n",
       " 'ask': 346,\n",
       " 'matter': 347,\n",
       " 'enough': 348,\n",
       " 'wake': 349,\n",
       " 'blue': 350,\n",
       " 'together': 351,\n",
       " 'pray': 352,\n",
       " 'forget': 353,\n",
       " 'comment': 354,\n",
       " 'body': 355,\n",
       " 'vote': 356,\n",
       " 'ppl': 357,\n",
       " 'state': 358,\n",
       " 'update': 359,\n",
       " 'h': 360,\n",
       " 'eye': 361,\n",
       " 'gift': 362,\n",
       " 'till': 363,\n",
       " 'porn': 364,\n",
       " 'usa': 365,\n",
       " 'reason': 366,\n",
       " 'tire': 367,\n",
       " 'trip': 368,\n",
       " 'reach': 369,\n",
       " 'poetry': 370,\n",
       " 'wrong': 371,\n",
       " 'almost': 372,\n",
       " 'le': 373,\n",
       " 'rain': 374,\n",
       " 'kind': 375,\n",
       " 'tbt': 376,\n",
       " 'train': 377,\n",
       " 'daddy': 378,\n",
       " 'choose': 379,\n",
       " 'sunshine': 380,\n",
       " 'cause': 381,\n",
       " 'write': 382,\n",
       " 'date': 383,\n",
       " 'victim': 384,\n",
       " 'parent': 385,\n",
       " 'gym': 386,\n",
       " 'coffee': 387,\n",
       " 'begin': 388,\n",
       " 'nude': 389,\n",
       " 'single': 390,\n",
       " 'depress': 391,\n",
       " 'anything': 392,\n",
       " 'fact': 393,\n",
       " 'others': 394,\n",
       " 'sign': 395,\n",
       " 'far': 396,\n",
       " 'sick': 397,\n",
       " 'allahsoil': 398,\n",
       " 'yesterday': 399,\n",
       " 'color': 400,\n",
       " 'thought': 401,\n",
       " 'direct': 402,\n",
       " 'bird': 403,\n",
       " 'prayfororlando': 404,\n",
       " 'spend': 405,\n",
       " 'forever': 406,\n",
       " 'light': 407,\n",
       " 'k': 408,\n",
       " 'lie': 409,\n",
       " 'side': 410,\n",
       " 'park': 411,\n",
       " 'else': 412,\n",
       " 'hand': 413,\n",
       " 'green': 414,\n",
       " 'point': 415,\n",
       " 'actually': 416,\n",
       " 'customer': 417,\n",
       " 'minute': 418,\n",
       " 'goal': 419,\n",
       " 'send': 420,\n",
       " 'sorry': 421,\n",
       " 'daughter': 422,\n",
       " 'sure': 423,\n",
       " 'goodmorning': 424,\n",
       " 'name': 425,\n",
       " 'social': 426,\n",
       " 'tuesday': 427,\n",
       " 'suppo': 428,\n",
       " 'bed': 429,\n",
       " 'style': 430,\n",
       " 'special': 431,\n",
       " 'lifestyle': 432,\n",
       " 'gbp': 433,\n",
       " 'human': 434,\n",
       " 'early': 435,\n",
       " 'animal': 436,\n",
       " 'f': 437,\n",
       " 'uk': 438,\n",
       " 'follower': 439,\n",
       " 'nature': 440,\n",
       " 'seem': 441,\n",
       " 'act': 442,\n",
       " 'anymore': 443,\n",
       " 'gorilla': 444,\n",
       " 'muslim': 445,\n",
       " 'lunch': 446,\n",
       " 'hardcore': 447,\n",
       " 'nervous': 448,\n",
       " 'hold': 449,\n",
       " 'star': 450,\n",
       " 'lgbt': 451,\n",
       " 'sit': 452,\n",
       " 'release': 453,\n",
       " 'cold': 454,\n",
       " 'death': 455,\n",
       " 'design': 456,\n",
       " 'favorite': 457,\n",
       " 'hit': 458,\n",
       " 'prayer': 459,\n",
       " 'saw': 460,\n",
       " 'dominate': 461,\n",
       " 'sister': 462,\n",
       " 'app': 463,\n",
       " 'yr': 464,\n",
       " 'view': 465,\n",
       " 'lady': 466,\n",
       " 'hello': 467,\n",
       " 'episode': 468,\n",
       " 'speak': 469,\n",
       " 'easy': 470,\n",
       " 'understand': 471,\n",
       " 'complete': 472,\n",
       " 'garden': 473,\n",
       " 'vacation': 474,\n",
       " 'film': 475,\n",
       " 'grow': 476,\n",
       " 'red': 477,\n",
       " 'bite': 478,\n",
       " 'student': 479,\n",
       " 'guess': 480,\n",
       " 'surprise': 481,\n",
       " 'yeah': 482,\n",
       " 'lead': 483,\n",
       " 'hu': 484,\n",
       " 'present': 485,\n",
       " 'sometimes': 486,\n",
       " 'brother': 487,\n",
       " 'chill': 488,\n",
       " 'instagram': 489,\n",
       " 'usd': 490,\n",
       " 'daily': 491,\n",
       " 'history': 492,\n",
       " 'football': 493,\n",
       " 'pas': 494,\n",
       " 'dear': 495,\n",
       " 'conference': 496,\n",
       " 'joke': 497,\n",
       " 'fight': 498,\n",
       " 'environment': 499,\n",
       " 'final': 500,\n",
       " 'join': 501,\n",
       " 'group': 502,\n",
       " 'sunny': 503,\n",
       " 'bitch': 504,\n",
       " 'visit': 505,\n",
       " 'lucky': 506,\n",
       " 'model': 507,\n",
       " 'fly': 508,\n",
       " 'treat': 509,\n",
       " 'florida': 510,\n",
       " 'power': 511,\n",
       " 'tv': 512,\n",
       " 'sing': 513,\n",
       " 'might': 514,\n",
       " 'nd': 515,\n",
       " 'memory': 516,\n",
       " 'ok': 517,\n",
       " 'depression': 518,\n",
       " 'whole': 519,\n",
       " 'cheer': 520,\n",
       " 'soul': 521,\n",
       " 'self': 522,\n",
       " 'experience': 523,\n",
       " 'dont': 524,\n",
       " 'buffalo': 525,\n",
       " 'stas': 526,\n",
       " 'july': 527,\n",
       " 'photography': 528,\n",
       " 'thankyou': 529,\n",
       " 'ago': 530,\n",
       " 'stand': 531,\n",
       " 'everyday': 532,\n",
       " 'club': 533,\n",
       " 'tgif': 534,\n",
       " 'anniversary': 535,\n",
       " 'rock': 536,\n",
       " 'fall': 537,\n",
       " 'though': 538,\n",
       " 'launch': 539,\n",
       " 'welcome': 540,\n",
       " 'c': 541,\n",
       " 'problem': 542,\n",
       " 'as': 543,\n",
       " 'racism': 544,\n",
       " 'future': 545,\n",
       " 'youtube': 546,\n",
       " 'service': 547,\n",
       " 'president': 548,\n",
       " 'simulator': 549,\n",
       " 'adapt': 550,\n",
       " 'fail': 551,\n",
       " 'cant': 552,\n",
       " 'heal': 553,\n",
       " 'disney': 554,\n",
       " 'picoftheday': 555,\n",
       " 'control': 556,\n",
       " 'snapchat': 557,\n",
       " 'la': 558,\n",
       " 'empty': 559,\n",
       " 'line': 560,\n",
       " 'anyone': 561,\n",
       " 'realize': 562,\n",
       " 'test': 563,\n",
       " 'nyc': 564,\n",
       " 'police': 565,\n",
       " 'friendship': 566,\n",
       " 'staing': 567,\n",
       " 'safe': 568,\n",
       " 'retweet': 569,\n",
       " 'damn': 570,\n",
       " 'add': 571,\n",
       " 'boyfriend': 572,\n",
       " 'breakfast': 573,\n",
       " 'na': 574,\n",
       " 'mother': 575,\n",
       " 'list': 576,\n",
       " 'decide': 577,\n",
       " 'hr': 578,\n",
       " 'low': 579,\n",
       " 'cake': 580,\n",
       " 'disappoint': 581,\n",
       " 'create': 582,\n",
       " 'water': 583,\n",
       " 'class': 584,\n",
       " 'simple': 585,\n",
       " 'ht': 586,\n",
       " 'respect': 587,\n",
       " 'afternoon': 588,\n",
       " 'half': 589,\n",
       " 'voice': 590,\n",
       " 'inspire': 591,\n",
       " 'queen': 592,\n",
       " 'tragedy': 593,\n",
       " 'able': 594,\n",
       " 'phone': 595,\n",
       " 'brexit': 596,\n",
       " 'mad': 597,\n",
       " 'sky': 598,\n",
       " 'conce': 599,\n",
       " 'impoant': 600,\n",
       " 'idea': 601,\n",
       " 'count': 602,\n",
       " 'yoga': 603,\n",
       " 'beer': 604,\n",
       " 'question': 605,\n",
       " 'cannot': 606,\n",
       " 'choice': 607,\n",
       " 'ramadan': 608,\n",
       " 'small': 609,\n",
       " 'hat': 610,\n",
       " 'adventure': 611,\n",
       " 'culture': 612,\n",
       " 'vine': 613,\n",
       " 'htt': 614,\n",
       " 'agree': 615,\n",
       " 'makeup': 616,\n",
       " 'kick': 617,\n",
       " 'trend': 618,\n",
       " 'l': 619,\n",
       " 'teen': 620,\n",
       " 'cover': 621,\n",
       " 'account': 622,\n",
       " 'fresh': 623,\n",
       " 'web': 624,\n",
       " 'stuff': 625,\n",
       " 'step': 626,\n",
       " 'truly': 627,\n",
       " 'dinner': 628,\n",
       " 'card': 629,\n",
       " 'sale': 630,\n",
       " 'target': 631,\n",
       " 'pathetic': 632,\n",
       " 'smh': 633,\n",
       " 'wanna': 634,\n",
       " 'camp': 635,\n",
       " 'build': 636,\n",
       " 'fast': 637,\n",
       " 'least': 638,\n",
       " 'fit': 639,\n",
       " 'wife': 640,\n",
       " 'save': 641,\n",
       " 'cantwait': 642,\n",
       " 'ride': 643,\n",
       " 'fire': 644,\n",
       " 'india': 645,\n",
       " 'mine': 646,\n",
       " 'beat': 647,\n",
       " 'maybe': 648,\n",
       " 'sell': 649,\n",
       " 'festival': 650,\n",
       " 'sound': 651,\n",
       " 'nigga': 652,\n",
       " 'leadership': 653,\n",
       " 'dark': 654,\n",
       " 'pack': 655,\n",
       " 'community': 656,\n",
       " 'simulation': 657,\n",
       " 'issue': 658,\n",
       " 'fear': 659,\n",
       " 'flight': 660,\n",
       " 'pain': 661,\n",
       " 'england': 662,\n",
       " 'cut': 663,\n",
       " 'draw': 664,\n",
       " 'wet': 665,\n",
       " 'shock': 666,\n",
       " 'politics': 667,\n",
       " 'mr': 668,\n",
       " 'expect': 669,\n",
       " 'violence': 670,\n",
       " 'different': 671,\n",
       " 'mindset': 672,\n",
       " 'course': 673,\n",
       " 'room': 674,\n",
       " 'mass': 675,\n",
       " 'client': 676,\n",
       " 'drive': 677,\n",
       " 'oitnb': 678,\n",
       " 'murder': 679,\n",
       " 'dj': 680,\n",
       " 'wonder': 681,\n",
       " 'orlandoshooting': 682,\n",
       " 'officially': 683,\n",
       " 'award': 684,\n",
       " 'worry': 685,\n",
       " 'street': 686,\n",
       " 'hero': 687,\n",
       " 'education': 688,\n",
       " 'actor': 689,\n",
       " 'relationship': 690,\n",
       " 'bag': 691,\n",
       " 'age': 692,\n",
       " 'message': 693,\n",
       " 'fridayfeeling': 694,\n",
       " 'hillary': 695,\n",
       " 'sea': 696,\n",
       " 'pick': 697,\n",
       " 'local': 698,\n",
       " 'shoe': 699,\n",
       " 'tragic': 700,\n",
       " 'lonely': 701,\n",
       " 'instalike': 702,\n",
       " 'dress': 703,\n",
       " 'rather': 704,\n",
       " 'national': 705,\n",
       " 'gop': 706,\n",
       " 'folk': 707,\n",
       " 'swim': 708,\n",
       " 'reality': 709,\n",
       " 'glad': 710,\n",
       " 'wtf': 711,\n",
       " 'content': 712,\n",
       " 'nbafinals': 713,\n",
       " 'g': 714,\n",
       " 'receive': 715,\n",
       " 'three': 716,\n",
       " 'survive': 717,\n",
       " 'pop': 718,\n",
       " 'past': 719,\n",
       " 'france': 720,\n",
       " 'deserve': 721,\n",
       " 'continue': 722,\n",
       " 'blonde': 723,\n",
       " 'wear': 724,\n",
       " 'product': 725,\n",
       " 'japan': 726,\n",
       " 'tag': 727,\n",
       " 'fantastic': 728,\n",
       " 'tip': 729,\n",
       " 'war': 730,\n",
       " 'market': 731,\n",
       " 'islam': 732,\n",
       " 'freedom': 733,\n",
       " 'law': 734,\n",
       " 'stupid': 735,\n",
       " 'office': 736,\n",
       " 'pink': 737,\n",
       " 'workout': 738,\n",
       " 'poor': 739,\n",
       " 'xxx': 740,\n",
       " 'apple': 741,\n",
       " 'liberal': 742,\n",
       " 'bday': 743,\n",
       " 'tune': 744,\n",
       " 'catch': 745,\n",
       " 'weather': 746,\n",
       " 'blur': 747,\n",
       " 'emotion': 748,\n",
       " 'announce': 749,\n",
       " 'gorgeous': 750,\n",
       " 'instamood': 751,\n",
       " 'official': 752,\n",
       " 'husband': 753,\n",
       " 'marry': 754,\n",
       " 'result': 755,\n",
       " 'puppy': 756,\n",
       " 'deep': 757,\n",
       " 'instead': 758,\n",
       " 'likelike': 759,\n",
       " 'campaign': 760,\n",
       " 'congrats': 761,\n",
       " 'member': 762,\n",
       " 'despite': 763,\n",
       " 'scar': 764,\n",
       " 'blogger': 765,\n",
       " 'celebration': 766,\n",
       " 'review': 767,\n",
       " 'spos': 768,\n",
       " 'second': 769,\n",
       " 'ff': 770,\n",
       " 'vegan': 771,\n",
       " 'vibe': 772,\n",
       " 'ice': 773,\n",
       " 'page': 774,\n",
       " 'ahead': 775,\n",
       " 'bore': 776,\n",
       " 'busy': 777,\n",
       " 'sho': 778,\n",
       " 'college': 779,\n",
       " 'record': 780,\n",
       " 'unite': 781,\n",
       " 'waste': 782,\n",
       " 'clean': 783,\n",
       " 'paint': 784,\n",
       " 'road': 785,\n",
       " 'king': 786,\n",
       " 'hi': 787,\n",
       " 'congratulation': 788,\n",
       " 'loss': 789,\n",
       " 'drop': 790,\n",
       " 'staed': 791,\n",
       " 'land': 792,\n",
       " 'return': 793,\n",
       " 'co': 794,\n",
       " 'trust': 795,\n",
       " 'disgust': 796,\n",
       " 'facebook': 797,\n",
       " 'energy': 798,\n",
       " 'pride': 799,\n",
       " 'confuse': 800,\n",
       " 'terrorist': 801,\n",
       " 'brand': 802,\n",
       " 'hungry': 803,\n",
       " 'along': 804,\n",
       " 'heabroken': 805,\n",
       " 'wine': 806,\n",
       " 'luck': 807,\n",
       " 'seriously': 808,\n",
       " 'series': 809,\n",
       " 'answer': 810,\n",
       " 'offer': 811,\n",
       " 'deal': 812,\n",
       " 'interest': 813,\n",
       " 'allow': 814,\n",
       " 'project': 815,\n",
       " 'teach': 816,\n",
       " 'haha': 817,\n",
       " 'pool': 818,\n",
       " 'gotta': 819,\n",
       " 'behind': 820,\n",
       " 'pizza': 821,\n",
       " 'pussy': 822,\n",
       " 'tech': 823,\n",
       " 'da': 824,\n",
       " 'action': 825,\n",
       " 'countdown': 826,\n",
       " 'store': 827,\n",
       " 'fake': 828,\n",
       " 'blame': 829,\n",
       " 'hell': 830,\n",
       " 'bike': 831,\n",
       " 'glass': 832,\n",
       " 'hill': 833,\n",
       " 'company': 834,\n",
       " 'inshot': 835,\n",
       " 'tour': 836,\n",
       " 'woh': 837,\n",
       " 'mountain': 838,\n",
       " 'sunset': 839,\n",
       " 'female': 840,\n",
       " 'appreciate': 841,\n",
       " 'staff': 842,\n",
       " 'idiot': 843,\n",
       " 'nasty': 844,\n",
       " 'notice': 845,\n",
       " 'block': 846,\n",
       " 'germany': 847,\n",
       " 'stick': 848,\n",
       " 'nba': 849,\n",
       " 'hatred': 850,\n",
       " 'million': 851,\n",
       " 'nation': 852,\n",
       " 'xx': 853,\n",
       " 'link': 854,\n",
       " 'secret': 855,\n",
       " 'online': 856,\n",
       " 'key': 857,\n",
       " 'kiss': 858,\n",
       " 'everybody': 859,\n",
       " 'miami': 860,\n",
       " 'weak': 861,\n",
       " 'oil': 862,\n",
       " 'political': 863,\n",
       " 'album': 864,\n",
       " 'due': 865,\n",
       " 'fill': 866,\n",
       " 'christmas': 867,\n",
       " 'cook': 868,\n",
       " 'accept': 869,\n",
       " 'chance': 870,\n",
       " 'pet': 871,\n",
       " 'ball': 872,\n",
       " 'shame': 873,\n",
       " 'asian': 874,\n",
       " 'remain': 875,\n",
       " 'nail': 876,\n",
       " 'either': 877,\n",
       " 'yo': 878,\n",
       " 'naked': 879,\n",
       " 'internet': 880,\n",
       " 'church': 881,\n",
       " 'favourite': 882,\n",
       " 'nobody': 883,\n",
       " 'vega': 884,\n",
       " 'anti': 885,\n",
       " 'hop': 886,\n",
       " 'york': 887,\n",
       " 'upset': 888,\n",
       " 'case': 889,\n",
       " 'melancholy': 890,\n",
       " 'town': 891,\n",
       " 'cavs': 892,\n",
       " 'foot': 893,\n",
       " 'previous': 894,\n",
       " 'absolutely': 895,\n",
       " 'balance': 896,\n",
       " 'poem': 897,\n",
       " 'prove': 898,\n",
       " 'dory': 899,\n",
       " 'roll': 900,\n",
       " 'dude': 901,\n",
       " 'paris': 902,\n",
       " 'aist': 903,\n",
       " 'donald': 904,\n",
       " 'gif': 905,\n",
       " 'study': 906,\n",
       " 'pre': 907,\n",
       " 'instadaily': 908,\n",
       " 'suck': 909,\n",
       " 'horrible': 910,\n",
       " 'bc': 911,\n",
       " 'rd': 912,\n",
       " 'yummy': 913,\n",
       " 'august': 914,\n",
       " 'min': 915,\n",
       " 'session': 916,\n",
       " 'number': 917,\n",
       " 'okay': 918,\n",
       " 'nofilter': 919,\n",
       " 'space': 920,\n",
       " 'hang': 921,\n",
       " 'bar': 922,\n",
       " 'youth': 923,\n",
       " 'goodnight': 924,\n",
       " 'task': 925,\n",
       " 'inspirational': 926,\n",
       " 'calm': 927,\n",
       " 'sense': 928,\n",
       " 'rise': 929,\n",
       " 'near': 930,\n",
       " 'girlfriend': 931,\n",
       " 'probably': 932,\n",
       " 'loveit': 933,\n",
       " 'eur': 934,\n",
       " 'chase': 935,\n",
       " 'source': 936,\n",
       " 'throw': 937,\n",
       " 'wild': 938,\n",
       " 'slut': 939,\n",
       " 'player': 940,\n",
       " 'emotional': 941,\n",
       " 'huge': 942,\n",
       " 'box': 943,\n",
       " 'email': 944,\n",
       " 'faith': 945,\n",
       " 'california': 946,\n",
       " 'melancholymusic': 947,\n",
       " 'board': 948,\n",
       " 'ex': 949,\n",
       " 'anger': 950,\n",
       " 'development': 951,\n",
       " 'coach': 952,\n",
       " 'hotel': 953,\n",
       " 'piece': 954,\n",
       " 'udtapunjab': 955,\n",
       " 'exam': 956,\n",
       " 'literally': 957,\n",
       " 'spain': 958,\n",
       " 'especially': 959,\n",
       " 'focus': 960,\n",
       " 'naughty': 961,\n",
       " 'exactly': 962,\n",
       " 'gratitude': 963,\n",
       " 'match': 964,\n",
       " 'australia': 965,\n",
       " 'inside': 966,\n",
       " 'decision': 967,\n",
       " 'goodvibes': 968,\n",
       " 'hug': 969,\n",
       " 'towards': 970,\n",
       " 'positivity': 971,\n",
       " 'leader': 972,\n",
       " 'clothe': 973,\n",
       " 'stream': 974,\n",
       " 'speech': 975,\n",
       " 'outside': 976,\n",
       " 'repost': 977,\n",
       " 'term': 978,\n",
       " 'ibiza': 979,\n",
       " 'ripchristina': 980,\n",
       " 'shy': 981,\n",
       " 'south': 982,\n",
       " 'eah': 983,\n",
       " 'public': 984,\n",
       " 'alive': 985,\n",
       " 'type': 986,\n",
       " 'stage': 987,\n",
       " 'successful': 988,\n",
       " 'totally': 989,\n",
       " 'generation': 990,\n",
       " 'judge': 991,\n",
       " 'entire': 992,\n",
       " 'graduation': 993,\n",
       " 'motivate': 994,\n",
       " 'double': 995,\n",
       " 'door': 996,\n",
       " 'foodie': 997,\n",
       " 'blm': 998,\n",
       " 'christian': 999,\n",
       " 'tattoo': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e9d98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokenized=tokenizer.texts_to_matrix(x_train,mode='binary')\n",
    "x_test_tokenized=tokenizer.texts_to_matrix(x_test,mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df51bfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>liv know weird yeahhh bread get hole stoner sistabanta</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one today believe bihday timeflies</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel sunday family travel couple family life chiangmai thailand</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure time day leisure leisuretime bangsean milksshop cocoa bos</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love findingdory wait movie even find dory</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thankful taxi thankful positive</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life measure number breath take moment take breath away maya angelou love woman</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everyday caturday caturday cat saturday indraloka weekend</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reason millennials work keith breene user challenge employment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yep definitely mean meanmuggin stare cat animallovers lol lmao bruh</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26577 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0     1     2     3     \\\n",
       "liv know weird yeahhh bread get hole stoner sis...   0.0   1.0   0.0   0.0   \n",
       "one today believe bihday timeflies                   0.0   1.0   0.0   0.0   \n",
       "travel sunday family travel couple family life ...   0.0   0.0   0.0   0.0   \n",
       "leisure time day leisure leisuretime bangsean m...   0.0   1.0   0.0   1.0   \n",
       "love findingdory wait movie even find dory           0.0   0.0   0.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "thankful taxi thankful positive                      0.0   0.0   0.0   0.0   \n",
       "life measure number breath take moment take bre...   0.0   1.0   0.0   0.0   \n",
       "everyday caturday caturday cat saturday indralo...   0.0   1.0   0.0   0.0   \n",
       "reason millennials work keith breene user chall...   0.0   1.0   1.0   0.0   \n",
       "yep definitely mean meanmuggin stare cat animal...   0.0   1.0   0.0   0.0   \n",
       "\n",
       "                                                    4     5     6     7     \\\n",
       "liv know weird yeahhh bread get hole stoner sis...   0.0   1.0   0.0   0.0   \n",
       "one today believe bihday timeflies                   0.0   0.0   0.0   0.0   \n",
       "travel sunday family travel couple family life ...   0.0   0.0   0.0   0.0   \n",
       "leisure time day leisure leisuretime bangsean m...   0.0   0.0   0.0   0.0   \n",
       "love findingdory wait movie even find dory           1.0   0.0   0.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "thankful taxi thankful positive                      0.0   0.0   0.0   0.0   \n",
       "life measure number breath take moment take bre...   1.0   0.0   0.0   0.0   \n",
       "everyday caturday caturday cat saturday indralo...   0.0   0.0   0.0   0.0   \n",
       "reason millennials work keith breene user chall...   0.0   0.0   0.0   0.0   \n",
       "yep definitely mean meanmuggin stare cat animal...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                                    8     9     ...  9990  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...   0.0   0.0  ...   0.0   \n",
       "one today believe bihday timeflies                   0.0   0.0  ...   0.0   \n",
       "travel sunday family travel couple family life ...   0.0   0.0  ...   0.0   \n",
       "leisure time day leisure leisuretime bangsean m...   0.0   0.0  ...   0.0   \n",
       "love findingdory wait movie even find dory           0.0   0.0  ...   0.0   \n",
       "...                                                  ...   ...  ...   ...   \n",
       "thankful taxi thankful positive                      0.0   0.0  ...   0.0   \n",
       "life measure number breath take moment take bre...   0.0   0.0  ...   0.0   \n",
       "everyday caturday caturday cat saturday indralo...   0.0   0.0  ...   0.0   \n",
       "reason millennials work keith breene user chall...   0.0   0.0  ...   0.0   \n",
       "yep definitely mean meanmuggin stare cat animal...   0.0   0.0  ...   0.0   \n",
       "\n",
       "                                                    9991  9992  9993  9994  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...   0.0   0.0   0.0   0.0   \n",
       "one today believe bihday timeflies                   0.0   0.0   0.0   0.0   \n",
       "travel sunday family travel couple family life ...   0.0   0.0   0.0   0.0   \n",
       "leisure time day leisure leisuretime bangsean m...   0.0   0.0   0.0   0.0   \n",
       "love findingdory wait movie even find dory           0.0   0.0   0.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "thankful taxi thankful positive                      0.0   0.0   0.0   0.0   \n",
       "life measure number breath take moment take bre...   0.0   0.0   0.0   0.0   \n",
       "everyday caturday caturday cat saturday indralo...   0.0   0.0   0.0   0.0   \n",
       "reason millennials work keith breene user chall...   0.0   0.0   0.0   0.0   \n",
       "yep definitely mean meanmuggin stare cat animal...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                                    9995  9996  9997  9998  \\\n",
       "liv know weird yeahhh bread get hole stoner sis...   0.0   0.0   0.0   0.0   \n",
       "one today believe bihday timeflies                   0.0   0.0   0.0   0.0   \n",
       "travel sunday family travel couple family life ...   0.0   0.0   0.0   0.0   \n",
       "leisure time day leisure leisuretime bangsean m...   0.0   0.0   0.0   0.0   \n",
       "love findingdory wait movie even find dory           0.0   0.0   0.0   0.0   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "thankful taxi thankful positive                      0.0   0.0   0.0   0.0   \n",
       "life measure number breath take moment take bre...   0.0   0.0   0.0   0.0   \n",
       "everyday caturday caturday cat saturday indralo...   0.0   0.0   0.0   0.0   \n",
       "reason millennials work keith breene user chall...   0.0   0.0   0.0   0.0   \n",
       "yep definitely mean meanmuggin stare cat animal...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                                    9999  \n",
       "liv know weird yeahhh bread get hole stoner sis...   0.0  \n",
       "one today believe bihday timeflies                   0.0  \n",
       "travel sunday family travel couple family life ...   0.0  \n",
       "leisure time day leisure leisuretime bangsean m...   0.0  \n",
       "love findingdory wait movie even find dory           0.0  \n",
       "...                                                  ...  \n",
       "thankful taxi thankful positive                      0.0  \n",
       "life measure number breath take moment take bre...   0.0  \n",
       "everyday caturday caturday cat saturday indralo...   0.0  \n",
       "reason millennials work keith breene user chall...   0.0  \n",
       "yep definitely mean meanmuggin stare cat animal...   0.0  \n",
       "\n",
       "[26577 rows x 10000 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train_tokenized,index=x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6820b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "model_2=models.Sequential()\n",
    "model_2.add(layers.Dense(128,activation='relu',input_shape=(10000,)))\n",
    "model_2.add(layers.Dense(64,activation='relu'))\n",
    "model_2.add(layers.Dense(32,activation='relu'))\n",
    "model_2.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf0e11e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Honda\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "model_2.compile(optimizer= RMSprop(lr=0.0005),\n",
    "              loss= keras.losses.binary_crossentropy,\n",
    "              metrics= [keras.metrics.binary_accuracy,keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a835cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "831/831 [==============================] - 15s 17ms/step - loss: 0.2101 - binary_accuracy: 0.9345 - auc_1: 0.8114 - val_loss: 0.1822 - val_binary_accuracy: 0.9441 - val_auc_1: 0.8525\n",
      "Epoch 2/10\n",
      "831/831 [==============================] - 14s 17ms/step - loss: 0.1478 - binary_accuracy: 0.9536 - auc_1: 0.9041 - val_loss: 0.1663 - val_binary_accuracy: 0.9523 - val_auc_1: 0.8678\n",
      "Epoch 3/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.1208 - binary_accuracy: 0.9637 - auc_1: 0.9293 - val_loss: 0.1537 - val_binary_accuracy: 0.9550 - val_auc_1: 0.8903\n",
      "Epoch 4/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.1007 - binary_accuracy: 0.9705 - auc_1: 0.9452 - val_loss: 0.1517 - val_binary_accuracy: 0.9567 - val_auc_1: 0.9051\n",
      "Epoch 5/10\n",
      "831/831 [==============================] - 13s 16ms/step - loss: 0.0852 - binary_accuracy: 0.9764 - auc_1: 0.9570 - val_loss: 0.1683 - val_binary_accuracy: 0.9539 - val_auc_1: 0.9093\n",
      "Epoch 6/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.0738 - binary_accuracy: 0.9803 - auc_1: 0.9615 - val_loss: 0.1637 - val_binary_accuracy: 0.9546 - val_auc_1: 0.9119\n",
      "Epoch 7/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.0645 - binary_accuracy: 0.9833 - auc_1: 0.9679 - val_loss: 0.1784 - val_binary_accuracy: 0.9536 - val_auc_1: 0.9120\n",
      "Epoch 8/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.0572 - binary_accuracy: 0.9860 - auc_1: 0.9714 - val_loss: 0.1907 - val_binary_accuracy: 0.9526 - val_auc_1: 0.9015\n",
      "Epoch 9/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.0512 - binary_accuracy: 0.9876 - auc_1: 0.9757 - val_loss: 0.1974 - val_binary_accuracy: 0.9523 - val_auc_1: 0.9098\n",
      "Epoch 10/10\n",
      "831/831 [==============================] - 14s 16ms/step - loss: 0.0460 - binary_accuracy: 0.9896 - auc_1: 0.9776 - val_loss: 0.2062 - val_binary_accuracy: 0.9539 - val_auc_1: 0.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14bfd4ae5e0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train_tokenized,y_train,epochs=10,validation_data=(x_test_tokenized,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454fc52",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0ef4f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9cf70e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_v=np.zeros((len(x_train),300))\n",
    "x_test_v=np.zeros((len(x_test),300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "053c3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(nlp.pipe(x_train)):\n",
    "    x_train_v[i, :] = doc.vector\n",
    "\n",
    "for i, doc in enumerate(nlp.pipe(x_test)):\n",
    "    x_test_v[i, :] = doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a233a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models\n",
    "model_3=models.Sequential()\n",
    "model_3.add(layers.Dense(128,activation='relu',input_shape=(300,)))\n",
    "model_3.add(layers.Dense(64,activation='relu'))\n",
    "model_3.add(layers.Dense(32,activation='relu'))\n",
    "model_3.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fbd5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "model_3.compile(optimizer= RMSprop(lr=0.0005),\n",
    "              loss= keras.losses.binary_crossentropy,\n",
    "              metrics= [keras.metrics.binary_accuracy,keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebfc1a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "831/831 [==============================] - 3s 3ms/step - loss: 0.1914 - binary_accuracy: 0.9368 - auc_2: 0.8398 - val_loss: 0.1551 - val_binary_accuracy: 0.9451 - val_auc_2: 0.9060\n",
      "Epoch 2/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1624 - binary_accuracy: 0.9444 - auc_2: 0.8903 - val_loss: 0.1479 - val_binary_accuracy: 0.9475 - val_auc_2: 0.9167\n",
      "Epoch 3/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1535 - binary_accuracy: 0.9474 - auc_2: 0.9053 - val_loss: 0.1549 - val_binary_accuracy: 0.9468 - val_auc_2: 0.9132\n",
      "Epoch 4/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1474 - binary_accuracy: 0.9492 - auc_2: 0.9131 - val_loss: 0.1409 - val_binary_accuracy: 0.9516 - val_auc_2: 0.9230\n",
      "Epoch 5/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1414 - binary_accuracy: 0.9510 - auc_2: 0.9210 - val_loss: 0.1433 - val_binary_accuracy: 0.9506 - val_auc_2: 0.9268\n",
      "Epoch 6/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1362 - binary_accuracy: 0.9528 - auc_2: 0.9267 - val_loss: 0.1453 - val_binary_accuracy: 0.9523 - val_auc_2: 0.9203\n",
      "Epoch 7/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1303 - binary_accuracy: 0.9559 - auc_2: 0.9325 - val_loss: 0.1448 - val_binary_accuracy: 0.9529 - val_auc_2: 0.9306\n",
      "Epoch 8/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1249 - binary_accuracy: 0.9577 - auc_2: 0.9368 - val_loss: 0.1465 - val_binary_accuracy: 0.9529 - val_auc_2: 0.9263\n",
      "Epoch 9/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1196 - binary_accuracy: 0.9609 - auc_2: 0.9420 - val_loss: 0.1492 - val_binary_accuracy: 0.9506 - val_auc_2: 0.9272\n",
      "Epoch 10/10\n",
      "831/831 [==============================] - 2s 2ms/step - loss: 0.1143 - binary_accuracy: 0.9612 - auc_2: 0.9477 - val_loss: 0.1594 - val_binary_accuracy: 0.9512 - val_auc_2: 0.9188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b38be42e0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(x_train_v,y_train,epochs=10,validation_data=(x_test_v,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c954e0",
   "metadata": {},
   "source": [
    "# CNN_N_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43f8e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_sentence(data):\n",
    "    max_len=0\n",
    "    for text in data:\n",
    "        text_len=len(text.split())\n",
    "        max_len=max(text_len,max_len)\n",
    "        \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5ab704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_input=get_longest_sentence(Data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f309d1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd7250a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6326803a435e4e079375e5d1083d48b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29530.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_emb = np.zeros((len(Data['tweet']), longest_input, 300))\n",
    "for i, text in enumerate(tqdm(nlp.pipe(Data['tweet']), total=len(Data['tweet']))):\n",
    "    for j, token in enumerate(text):\n",
    "        data_emb[i, j] = token.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff813a",
   "metadata": {},
   "source": [
    "#shape of data embedded (no of instances , max length of each sentence, 300 vct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dae8b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the network\n",
    "inputs = tf.keras.layers.Input((longest_input, 300))\n",
    "reshaped = tf.keras.layers.Reshape((longest_input, 300, 1))(inputs)\n",
    "\n",
    "\n",
    "filters = [2, 3, 4]\n",
    "\n",
    "# define the conv net\n",
    "conv_1 = tf.keras.layers.Conv2D(100, (filters[0], 300), activation='relu')(reshaped)\n",
    "conv_2 = tf.keras.layers.Conv2D(100, (filters[1], 300), activation='relu')(reshaped)\n",
    "conv_3 = tf.keras.layers.Conv2D(100, (filters[2], 300), activation='relu')(reshaped)\n",
    "\n",
    "# define max-pooling\n",
    "pool_1 = tf.keras.layers.MaxPooling2D((longest_input - filters[0] + 1, 1), strides=(1,1))(conv_1)\n",
    "pool_2 = tf.keras.layers.MaxPooling2D((longest_input - filters[1] + 1, 1), strides=(1,1))(conv_2)\n",
    "pool_3 = tf.keras.layers.MaxPooling2D((longest_input - filters[2] + 1, 1), strides=(1,1))(conv_3)\n",
    "\n",
    "# concatenate the convs\n",
    "merged_tensor = tf.keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "\n",
    "# now flatten them and add a dense layer\n",
    "flatten = tf.keras.layers.Flatten()(merged_tensor)\n",
    "\n",
    "# add a dense layer\n",
    "clf = tf.keras.layers.Dense(100, activation='relu')(flatten)\n",
    "\n",
    "# add final output\n",
    "clf = tf.keras.layers.Dense(1, activation='sigmoid')(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4c43ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 37, 300)]    0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 37, 300, 1)   0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 36, 1, 100)   60100       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 35, 1, 100)   90100       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 34, 1, 100)   120100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 1, 100)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 1, 100)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]',        \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 100)          30100       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            101         ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 300,501\n",
      "Trainable params: 300,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Model(inputs, clf)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc','AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34300cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_emb, Data['label'], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33cc8c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "739/739 [==============================] - 23s 29ms/step - loss: 0.0000e+00 - acc: 0.9309 - auc: 0.4980 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 2/10\n",
      "739/739 [==============================] - 23s 31ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 3/10\n",
      "739/739 [==============================] - 23s 31ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 4/10\n",
      "739/739 [==============================] - 25s 34ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 5/10\n",
      "739/739 [==============================] - 26s 35ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 6/10\n",
      "739/739 [==============================] - 24s 32ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 7/10\n",
      "739/739 [==============================] - 23s 31ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 8/10\n",
      "739/739 [==============================] - 23s 31ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 9/10\n",
      "739/739 [==============================] - 23s 32ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n",
      "Epoch 10/10\n",
      "739/739 [==============================] - 23s 32ms/step - loss: 0.0000e+00 - acc: 0.9321 - auc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 0.9306 - val_auc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b31acad30>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86966ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
